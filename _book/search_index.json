[
["index.html", "Statistics, Models and Machine Learning 1 Introduction 1.1 Motivation for this book 1.2 How to read this book 1.3 Notation and nomeclature used in this book 1.4 Supervised vs Unsupervised Models 1.5 Assessing Model Quality of Fit", " Statistics, Models and Machine Learning Michoel Snow 1 Introduction Should I be using linear regression, logistics regression, SVM, neural networks or one of at least a dozen other statistical/machine learning algorithms when trying to model my data? This is the fundamental question which has driven the authorship of this book. The problem is that the answer to that question for the most part is always the same, it depends. The answer depends not only on the data itself, but also why you are modeling in the first place, i.e., what question are you trying to answer with your model, as well as a whole host of other factors. The purpose of this book is to help you identify the important factors for choosing a model, given those factors which model(s) you should choose and why, and then finally how to implement that model in R or python. 1.1 Motivation for this book In undergrad I majored in bioengineering, then for grad school I started doing some basic modeling work in neuroscience. As I was only working with a few models, I really only needed to learn the inner workings of those specific models. However, after grad school when I was interviewing for jobs, they would often follow a similar pattern. Either I would be asked about which modeling approaches I felt comfortable with or about alternative modeling approaches for my research than the ones I used. To say I floundered would be a kind description of my response to these questions. It was obvious that to be competitive I needed to build a real foundation in modeling approaches. When I started reading books and taking courses, I ran into a different set of issues. The vast majority of these sources were written assuming that the learner was still in midst of schooling and had only recently taken courses like statistics and linear algebra. I had taken those courses over a decade before and my current knowledge of those fields was limited to the techniques that I was using at the time, and even for the the techniques I was still using, I had long forgotten their theoretical underpinnings. Thus this book tries to be complete in the sense that it only assumes the most basic knowledge of statistics and a bit of linear algebra, i.e., vector and matrix multiplication. That being said, I try to approach most concepts from multiple directions so even if vector and matrix operations are not your strong suit you should still come away with at least intuitive understanding of the presented topics. 1.2 How to read this book This book is long on purpose. I strove to make this book a one stop shop for all concepts related to the discussed topics, including multiple derivations and long winded discussions on what some might consider, simple ideas. This is intentional, and as such this book is more intended as a complete reference and I do not expect the average person to read the entire thing straight from start to finish. I have written it so that it can be read from start to finish but I expect most people to jump around to the topics that interest them and skip those that do not. To make this a little easier I have tried to be consistent in how material is presented. This book is broken up by model, ordered by computational complexity. Every chapter begins with a series of dataset visualizations and associated questions which are best answered through the models described in that chapter. Each chapter is intended to be modular and for the most part, you can read any of the chapters in any order. When multiple models utilize statistics or statistical techniques, the technique will be discussed the first time it is mentioned and all later mentions will refer the reader to where in the book they can find an explanation of that technique. 1.3 Notation and nomeclature used in this book We are all aware of the need for precision in mathematics and computation so as not to create unintentional errors, but I feel the same is true for language, especially in the description of mathematics and computation. For that reason I will strive to be consistent in my use of terminology, use the simplest possible accurate term and use as few terms as possible. For the sake of those who read other books on the subject, whenever a new term is introduced I will try to give all other commonly used names for that term. I will also bold a term whenever I give its definition. Note that when these rules disagree with standard agreed upon nomenclature, I will defer to the standard nomenclature, but will then explicitly define the terms to avoid confusion. Mathematical modeling lies at the heart of statistical learning, machine learning and any other name dreamt up by a marketing department. So instead of trying to define where one field ends and the other begins, or which technique is machine learning and which is statistics I am simply going to use the term model. In this book any relationship between variables is a model. A variable is a symbol which contains one or more known or unknown values. When the variable can contain any number of subset variables I will use the bold typeface and capital letters to represent it, such as \\(\\mathbf{X}\\) or \\(\\mathbf{Y}\\). Variables which contain no subset variables will be written using the regular typeface and capital letters, such as \\(X\\) or \\(Y\\). Individual values of a variable known as observations, also referred to as samples, will be written using a regular typeface and lowercase letters, such as \\(x\\) or \\(y\\). Indvidual observations for multiple variable will be written using the bold typeface and lowercase letters, such as \\(\\mathbf{x}\\) or \\(\\mathbf{y}\\). Numbered subscripts will refer to a specific subset or observation and lettered subscripts will be used for generic subsets or observations. When referring to both individual observations and subset variables, the first subscript is the observation and the second is the variable. The generic total number of observations is \\(n\\) and the generic total number of variables is \\(p\\). THis will make more sense when put together in an example. Let us say I am building a model which takes as its first four inputs the GPA, major, minor and high school of every student in Cornell’s graduating class of 1900. \\(\\mathbf{X}\\) refers to the set of all input variables. \\(X_1\\) refers to the GPA of all students and \\(X_2\\) refers to their major. \\(\\mathbf{x}_1\\) refers to all the inputs for the first student and \\(\\mathbf{x}_i\\) refers to the set of inputs for some generic \\(i\\)th student. \\(x_{3,1}\\) refers to the GPA for the third student. You can also think of this as a table where the rows are the observations and the columns are the subset variables. In matrix format this would look like: \\[\\mathbf{X} = \\left(X_1\\; X_2\\; X_3\\; \\ldots \\; X_p\\right)= \\left(\\begin{array} {c} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\mathbf{x}_3 \\\\ \\vdots \\\\ \\mathbf{x}_n \\end{array}\\right) = \\left(\\begin{array} {rrr} x_{1,1} &amp; x_{1,2} &amp; x_{1,3} &amp; \\ldots &amp; x_{1,p}\\\\ x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; \\ldots &amp; x_{2,p}\\\\ x_{3,1} &amp; x_{3,2} &amp; x_{3,3} &amp; \\ldots &amp; x_{3,p}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n,1} &amp; x_{n,2} &amp; x_{n,3} &amp; \\ldots &amp; x_{n,p}\\\\ \\end{array}\\right)\\] Unlike variables which are one or more unknowns, values are unknowns which always only take on a single value and will be represented with a lowercase letter, e.g., \\(a\\). Values which are associated with variables will share their subscripts, e.g., a value, \\(b\\), associated with variable \\(X_i\\), will be referenced as \\(b_i\\). A value associated with a specific variable is referred to as its coefficient. A function is a rule for taking an input and returning an output and is represented with a lower case letter followed by a pair of parentheses surrounding the input, e.g., \\(f(input)\\). Variables which are part of a function’s input are not suprisingly input variables, these are also referred to as indepedent variables, predictors or features. Similarly variables in the output will be termed output variables, but are also called dependent variables or responses. Using the model of a straight line, whose equation is \\[Y = mX + b\\] I can rewrite this as a function of the variable \\(X\\) as \\[ f(X) = mX + b\\] where \\(f(X) = Y\\). When building models often times we are only estimating the value of a coefficient or variable and don’t know its actual value. To distinguish estimates of variables and coefficients from their true counterparts, we put tiny hats on them, e.g., \\(\\hat{\\mathbf{Y}}\\), \\(\\hat{x}_{i,j}\\) or \\(\\hat{a}_0\\). 1.4 Supervised vs Unsupervised Models There are many ways to subdivide the various mathematical models, but for the topics covered within this book the dichotomy is made between supervised and unsupervised models. In Supervised models the data usually consists of observations where for every observation there is a paired input and output. In supervised modeling you are often given a training dataset which consists of entries for both the inputs and the outputs, as well as a test dataset, which only contains inputs from which you must predict outputs. The goal in supervised modeling is to create a function which accurately transforms the inputs into the outputs. In general the output only consists of a single variable but the input can consist of one or more variables. The standard nomenclature is to use one symbol for the output and one symbol for the input. In unsupervised learning the data is unlabeled, i.e., it is not split into inputs and outputs and the goal is to learn some desired feature about the data. A classic problem in unsupervised learning, is clustering, wherein the goal is to split the data into groups such that data within each group is more similar to each other than to data in other groups. 1.4.1 Solving Supervised Models In general, the goal of supervised models is to design a function which can estimate a desired output variable \\(\\mathbf{Y}\\), given a set of input variables \\(\\mathbf{X}\\), either for the puproses of predicting future outputs or for understanding the relationship between the inputs and the outputs. All supervised models assume that there is some relationship between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) such that the output variable \\(\\mathbf{Y}\\) is a function of some fixed, but unknown function of \\(\\mathbf{X}\\), written as \\(f(\\mathbf{X})\\): \\[\\begin{equation} \\mathbf{Y} \\approx f(\\mathbf{X}) \\tag{1.1} \\end{equation}\\] In equation (1.1), \\(\\mathbf{Y}\\) is only approximately equal to \\(f(\\mathbf{X})\\) because there almost always exists some random error in data. This is true whether we are talking about neural activity, weather patterns or economics. Nothing behaves exaclty as expected, instead there is always some random error. I can formalize this error by adding an error term, \\(\\epsilon\\), to equation (1.1). \\[\\begin{equation} \\mathbf{Y} = f(\\mathbf{X}) + \\epsilon \\tag{1.2} \\end{equation}\\] Where \\(\\epsilon\\) represents the inherent random error, is assumed to be indepedent and has an assumed mean of zero. In this case independence means that the error is unaffected by the input variable (even though this is not always true). Assuming a zero mean error implies that there is no inherent bias in the error, e.g., the error will not shift the data in one direction more than any others. The implication of \\(\\epsilon\\) is that even if we are able to perfectly estimate \\(f(\\mathbf{X})\\), we would be unable to predict \\(\\mathbf{Y}\\) with perfect accuracy due to the inherent error which is indepedent of, and so cannot be predicted by, \\(\\mathbf{X}\\). However, the problem is that while we assume this relationship exists, we do not know what the function \\(f(\\mathbf{X})\\) is. To that end, supervised models seek to estimate this function using data consisting of inputs values paied with corresponding output values. When used to estimated the parameters of a model, this dataset if often referred to as training data. This is in contrast to test data, which as the name states, is used to test the accuracy of the model on data different than what it was trained on. Using the estimate of the function learned from the training data we are able to make predictions of the output values given the input values in the test data. \\[\\begin{equation} \\hat{\\mathbf{Y}} = \\hat{f}(\\mathbf{X}) \\tag{1.3} \\end{equation}\\] Note that both the function and the output variable are indicated as estimates (through their tiny hats), but the input variable is not indicated as an estimate. This is because we know what the true inputs are we just don’t know what the function that relates the inputs to the outputs truly is and following from that we do not know what the output truly is for any given input, hence the tiny hats. The most common features which distinguish the different supervised models are as follows The mathematical structure of \\(f(\\mathbf{X})\\) The type of data that make up the output variable Estimation method of \\(f(\\mathbf{X})\\) 1.5 Assessing Model Quality of Fit 1.5.1 MSE ** START FROM HERE** However, note that unlike equation (1.2) When estimating the function \\(f()\\), there are two types of errors to consider % ggplot(aes(Age,Height)) + geom_jitter() +geom_line(data=NH_2,aes(Age,Height)) --> % mutate(Inc = 10000 + (Income-1)*5000) --> % ggplot(aes(Edu,Income)) + geom_point() --> "],
["sim-lin-reg.html", "2 Simple Linear Regression 2.1 Overview 2.2 Estimating the Coefficients 2.3 Model Accuracy 2.4 Variable parameters", " 2 Simple Linear Regression 2.1 Overview Almst all supervised models relate one or more input variables, \\(X_1,X_2, \\ldots , X_p\\), to an output variable, \\(Y\\), which can be generalized as \\[Y = f(X)\\] The goal of these models is to estimate the function \\(f(X)\\) with as much accuracy as possible. In a linear regression model, the assumption is that the input variables are related to the output variables through a linear combination, i.e., addition. The term regression refers to the fact that the output variable can take on any value. This is in contrast to classification models in which the output variable can take on one of a limited set of values. Classification models will be discussed in chapter NEED CHAPTER NAME. When the model is estimating the function for only a single input variable, it is termed simple linear regression and takes the form \\[\\begin{equation} Y = a_0 + a_1X \\tag{2.1} \\end{equation}\\] This equation might seem somewhat familiar to you, and by just replacing the letters used for the terms and switching around the order I end up with an equation which we all have encountered in grade school as the equation for a line \\[\\begin{equation} y = mx + b \\end{equation}\\] Who knew we were all doing a form of linear regression so many years ago. Even though the letters used for simple linear regression are different they have the same meaning. \\(a_0\\) is the intercept and \\(a_1\\) is the slope of the line. However, unlike in math class where the line perfectly fit the points we were given, when dealing with data points in the real world this is almost never true. This is the reason that I keep using the word estimate. The model might be close but there should always be the expectation that it will not prefectly fit the data. Additionally, if you try to perfectly fit the data you run the risk of overfitting your model, a topic which will be discussed in chapter NEED CHAPTER NAME. So when discussing models applied to real world data the equation for simple linear regression is often written as \\[\\begin{equation} Y \\approx a_0 + a_1 X \\tag{2.2} \\end{equation}\\] To understand why equation (2.2) is more appropriate than equation (2.1), let’s look at some data. Plotted in the figure below are average temperature readings at JFK airport from January through July of 2017. Using the least squares method described below I can estimate the linear regression coefficients and overlay the best fit line. You can see that there is no straight line which would hit all the points, so going back to my original assertion, even the most accurate model could only approximate the output variable given the input variables. Taking another step back, we have to remember that linear regression is a method of estimating the efficients of equation (2.2), which I can represent by giving the estimated coefficients and estimate output variables tiny hats. \\[\\begin{equation} \\hat{Y} = \\hat{a}_0 + \\hat{a}_1 X \\tag{2.3} \\end{equation}\\] 2.2 Estimating the Coefficients 2.2.1 Least squares The general idea of the least squares method is that you want to pick coefficents which minimize the differences between the given output and the output calculated from the right hand side of equation (2.3). This difference between the calculated output variable and the actual output variable is called the residual which is represented by the symbol \\(e\\). \\[\\begin{equation} e = Y-\\hat{Y} = Y - \\left(\\hat{a}_0 + \\hat{a}_1 X\\right) \\tag{2.4} \\end{equation}\\] The residuals in our previous plot are the distances from the points to the fitted line. If I then take the square of the residual for each observation and sum them all together I get the residual sum of squares (RSS). Minimizing the RSS is what makes this process “least squares”. \\[\\begin{equation} RSS = e_1^2 + e_2^2 + \\ldots + e_n^2 = \\sum\\limits_{i=1}^N e_i^2 \\tag{2.5} \\end{equation}\\] \\(e_i\\) represents the \\(i\\)th residual and refers to the difference between the \\(i\\)th actual output, \\(Y_i\\), and the \\(i\\)th predicted output, \\(\\hat{Y}_i\\). In the following section I am going to go, step by step, through two different methods of deriving the least squares coefficients. I find derivations useful for two reaons. The first is that they let you know where the equations come from. If you are reading this book then you are looking to understand modeling on a level deeper than simply, “how do I apply a certain model to my data”. The second reason that I like derivations is that you see the assumptions made in the derivations. This, in my opinion, is essential to being able to smartly use the different models, as you understand when the assumptions are valid and when they are not. There are multiple methods to solve for the minimum least squares coefficients, most dealing with variations on ordinary least squares using matrix algebra. I will be using matrix algebra to solve for the coefficients when there are multiple inputs but for simple linear regression the following derivation only requires knowledge of partial derivatives. Derivation 1 Before I start working through the derivation, there are two concepts I want to go through variance and covariance. The reason being that at the end of the derivation the estimate of slope, otherwise known as \\(a_1\\) can be calculated by dividing the covariance by the variance. When I get to that point I will discuss why this is the case, but I first wanted to make sure that you had a firm grasp on these concepts by themselves. Also, the in depth examination of variance will inform later parts of the derivation. The variance is a measure of spread for a variable, i.e., the distance of each value from the sample mean value. An intuitive example is shown below where the variance of the points on the targets increases from left to right. Mathematically the variance, in one dimension, is calculated as follows: \\[\\begin{equation} Var(x) = \\sigma^2 = \\frac{1}{N}\\sum\\limits_{i=1}^N \\left(x_i - \\mu\\right)\\left(x_i - \\mu\\right) = \\frac{1}{N}\\sum\\limits_{i=1}^N \\left(x_i - \\mu\\right)^2 \\tag{2.6} \\end{equation}\\] Where \\(\\mu_x\\) is the notation for the population mean value of x, which is discussed in the next paragraph. Squaring the difference serves two functions, the first is to equally weight positive and negative contributions to the variance. Without the square if a variable was evenly distributed around zero, regardless of its spread it would have a variance of zero. The second consequence is that as the distances increase the variance increases exponentially, as seen in the following graph: However, there is a problem with using equation (2.6), it requires that I already know the population mean. The population mean is the mean value given a set of numbers or values. The issue is that most of the time with data we are dealing with a small subsample of data and not the full set of data. For example, if I wanted to calculate the average rainfall in New York City yesterday, I would need to measure the rainfall in every location throughout the city. This is obviously impossible, which is why some people refer to the population mean as the omniscient mean. Instead, what I can do is measure the rainfall at certain points throughout the city and estimate the population mean using the sample mean, which is simply the mean calculated from the population sample collected. The difference between population mean and sample mean, might seem like simply a matter of semantics, but it has important ramifications and necessitates corrections. The variance using the sample mean, termed the sample variance, very similar to \\[\\begin{equation} Var(\\bar{x}) = s^2 = \\frac{1}{N-1}\\sum\\limits_{i=1}^N \\left(x_i - \\bar{x}\\right)\\left(x_i - \\bar{x}\\right) = \\frac{1}{N-1}\\sum\\limits_{i=1}^N \\left(x_i - \\bar{x}\\right)^2 \\tag{2.7} \\end{equation}\\] where \\(s^2\\) is the sample variance and \\(\\bar{x}\\) is the sample mean. For a full derivation of equation (2.7) see Population Vs. Sample Variances. Unlike the variance which is a measure of how a single variable varies with regards to itself, the covariance is a measure of how two variables, \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), vary with respect to each other. If the variables increase in value together and decrease together, i.e., when one variable increases the other variable increases as well, the covariance will be larger. If instead one variable increases while the other decreases then the covariance will be smaller. To actually calculate the covariance you can use the following formula: \\[\\begin{equation} Cov(x) = \\frac{1}{N}\\sum\\limits_{i=1}^N \\left(x_i - \\mu_x\\right)\\left(y_i - \\mu_y\\right) \\tag{2.8} \\end{equation}\\] If equation (2.8) looks suspiciously like equation (2.6), that’s because the formula for the variance is just a special case of the formula for the covariance when dealing with a single variable. To get an intuitive sense of what the covariance represents here are the covariances for a few different series of points. As you can see, as the slope of the line increases from the red to the green to the blue points, the covariance decreases. This is because for the red points, a change in \\(y\\) value of 5 roughly corresponds to a change in \\(x\\) value of 7 or 8, but for the green points, the same change in \\(y\\) of 5 corresponds to a change in \\(x\\) of only about 3 or 4. What this means is that for an increased slope the \\(x\\) values are changing at a slower rate and so this decreases their term in the covariance formulation. The purple points on the right show what happens to the covariance when the tyhe two variables move in opposite direction. The magnitutde is the same for the purple and red points, but while the red covariance is positive the purple is negative. This is because, with respect to each variable’s mean value, while \\(x\\) is positive, \\(y\\) is negative and vice versa. Just like with the variance, I have to distinguish between the population covariance, equation (2.8), and the sample covariance: \\[\\begin{equation} Cov(x) = \\frac{1}{N-1}\\sum\\limits_{i=1}^N \\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right) \\tag{2.9} \\end{equation}\\] The main difference once again is the replacement of \\(n-1\\) in the denominator in lieu of \\(n\\), for the same reason as the variance. Now that I have gone over the concepts of variance and covariance, I just need to go over some quick identities before I derive the least squares coefficients. \\[\\begin{equation} \\begin{split} \\frac{1}{N}\\sum\\limits_{i=1}^N y_i &amp; = \\bar{y} \\Longleftrightarrow \\sum\\limits_{i=1}^N y_i &amp; = N\\bar{y} \\\\ \\frac{1}{N}\\sum\\limits_{i=1}^N x_i &amp; = \\bar{x} \\Longleftrightarrow \\sum\\limits_{i=1}^N x_i &amp; = N\\bar{x} \\\\ \\end{split} \\tag{2.10} \\end{equation}\\] Equation (2.10) is just a simple variation on the definition of \\(\\bar{X}\\). The next two sets of identities are expansions of the variance and covariance equations and the equivalent ways they appear. I am going to be removing the denominator for the sake of simplicity, also because as you will soon see it will be divided out in the final derivation. \\[\\begin{align} \\sum\\limits_{i=1}^N (x_i-\\bar{x})(x_i - \\bar{x}) &amp;= \\sum\\limits_{i=1}^N x_i^2 - 2\\sum\\limits_{i=1}^N \\bar{x}x_i + \\sum\\limits_{i=1}^N \\bar{x}^2 \\\\ &amp;= \\sum\\limits_{i=1}^N x_i^2 - 2N\\bar{x}^2 + N\\bar{x}^2 \\\\ &amp;= \\sum\\limits_{i=1}^N x_i^2 - \\sum\\limits_{i=1}^N \\bar{x}x_i = \\sum\\limits_{i=1}^N x_i\\left(x_i-\\bar{x}\\right) \\tag{2.11} \\end{align}\\] Now repeating the process for the covariance: \\[\\begin{align} \\sum\\limits_{i=1}^N (x_i-\\bar{x})(y_i - \\bar{y}) &amp;= \\sum\\limits_{i=1}^N x_iy_i - \\sum\\limits_{i=1}^N \\bar{x}y_i - \\sum\\limits_{i=1}^N \\bar{y}x_i + \\sum\\limits_{i=1}^N\\bar{x}\\bar{y}\\\\ &amp;=\\sum\\limits_{i=1}^N x_iy_i - \\bar{x}\\sum\\limits_{i=1}^N y_i - \\bar{y} \\sum\\limits_{i=1}^N x_i + \\bar{x}\\bar{y}\\sum\\limits_{i=1}^N 1 \\\\ &amp;=\\sum\\limits_{i=1}^N x_iy_i -N\\bar{x}\\bar{y} - N\\bar{x}\\bar{y} + N\\bar{x}\\bar{y} \\\\ &amp;=\\sum\\limits_{i=1}^N x_iy_i - N\\bar{x}\\bar{y} \\\\ &amp;=\\sum\\limits_{i=1}^N x_iy_i - \\sum\\limits_{i=1}^N \\bar{y}x_i = \\sum\\limits_{i=1}^N x_i\\left(y_i - \\bar{y}\\right) \\tag{2.12}\\\\ &amp;= \\sum\\limits_{i=1}^N x_iy_i - \\sum\\limits_{i=1}^N \\bar{x}y_i = \\sum\\limits_{i=1}^N y_i\\left(x_i - \\bar{x}\\right) \\tag{2.13}\\\\ \\end{align}\\] This series of equations is simply going back and forth between equivalent values using the relationships established in equation (2.10). I can split and rejoin summations because the operations are all linear. At the end of the above series I end up with two relationships which will be needed later. Now with all these tools, let’s attack this derivation. If you remember back from your high school and college math classes, when we are trying to find the minimum value of a function we take the derivative, set it equal to zero and solve for the parameter(s) of interest. Thankfully, the process hasn’t changed since then. Also, since there are multiple parameters of interest, i.e., \\(\\hat{a_0}\\) and \\(\\hat{a_1}\\), I will need to use partial derivatives. First I want to rewrite equation (2.5) in a slightly more derivative friendly way. \\[\\begin{equation} S = \\sum\\limits_{i=1}^N \\left(y_i - \\hat{a}_0 + \\hat{a}_1 x_i \\right)^2 \\tag{2.14} \\end{equation}\\] Starting with \\(\\hat{a_0}\\) \\[\\begin{align} \\frac{\\partial S}{\\partial \\hat{a}_0} = -2 &amp;\\sum\\limits_{i=1}^N \\left(y_i - \\hat{a}_0 - \\hat{a_1}x_i\\right)\\\\ &amp;\\sum\\limits_{i=1}^N \\left(y_i - \\hat{a}_0 - \\hat{a_1}x_i\\right) = 0 \\\\ &amp;\\sum\\limits_{i=1}^N y_i - \\hat{a}_0 \\sum\\limits_{i=1}^N 1 - \\hat{a_1} \\sum\\limits_{i=1}^N x_i = 0 \\\\ &amp; N\\bar{y} - N\\hat{a}_0 - N\\hat{a_1}\\bar{x} = 0 \\\\ &amp; \\bar{y} - \\hat{a}_0 - \\hat{a_1}\\bar{x} = 0 \\\\ &amp; \\hat{a}_0 = \\bar{y} - \\hat{a_1}\\bar{x} \\tag{2.15} \\end{align}\\] The outcome all these steps is something that seems relatively simple, that the estimate of the intercept is just based on the estimated slope and the average input and output values. That being said it does make sense, as the intercept serves to horizontally shift the line. Once we have estimated the slope we want an intercept which positions the resulting line to go through the center of data as calculated by the average position. Now that I have estimated \\(\\hat{a}_0\\), I can estimate \\(\\hat{a}_1\\) \\[\\begin{align} \\frac{\\partial S}{\\partial \\hat{a}_1} = -2 &amp;\\sum\\limits_{i=1}^N x_i \\left(y_i - \\hat{a}_0 - \\hat{a_1}x_i\\right)\\\\ &amp;\\sum\\limits_{i=1}^N x_i \\left(y_i - \\hat{a}_0 - \\hat{a_1}x_i\\right) = 0 \\\\ &amp;\\sum\\limits_{i=1}^N x_iy_i - \\hat{a}_0 \\sum\\limits_{i=1}^N x_i - \\hat{a_1} \\sum\\limits_{i=1}^N x_i^2 = 0 \\\\ &amp;\\sum\\limits_{i=1}^N x_iy_i = \\hat{a}_0 \\sum\\limits_{i=1}^N x_i + \\hat{a_1} \\sum\\limits_{i=1}^N x_i^2 \\\\ &amp;\\sum\\limits_{i=1}^N x_iy_i = N\\hat{a}_0 \\bar{x} + \\hat{a_1} \\sum\\limits_{i=1}^N x_i^2 \\\\ &amp;\\sum\\limits_{i=1}^N x_iy_i = N\\left(\\bar{y} - \\hat{a_1}\\bar{x}\\right)\\bar{x} + \\hat{a_1} \\sum\\limits_{i=1}^N x_i^2 \\\\ &amp;\\sum\\limits_{i=1}^N x_iy_i = N\\bar{y}\\bar{x} - N\\hat{a_1}\\bar{x}^2 + \\hat{a_1} \\sum\\limits_{i=1}^N x_i^2 \\\\ &amp;\\sum\\limits_{i=1}^N x_iy_i = \\sum\\limits_{i=1}^N y_i \\bar{x} - \\hat{a_1}\\sum\\limits_{i=1}^N x_i \\bar{x} + \\hat{a_1} \\sum\\limits_{i=1}^N x_i^2 \\\\ &amp;\\sum\\limits_{i=1}^N x_iy_i - \\sum\\limits_{i=1}^N y_i \\bar{x} = \\hat{a_1} \\sum\\limits_{i=1}^N x_i^2 - \\hat{a_1}\\sum\\limits_{i=1}^N x_i \\bar{x} \\\\ &amp;\\sum\\limits_{i=1}^N y_i\\left(x_i - \\bar{x}\\right) = \\hat{a_1}\\sum\\limits_{i=1}^N x_i \\left(x_i - \\bar{x}\\right) \\\\ &amp;\\sum\\limits_{i=1}^N \\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right) = \\hat{a_1}\\sum\\limits_{i=1}^N \\left(x_i - \\bar{x}\\right)^2 \\\\ &amp;\\hat{a_1} = \\frac{\\sum\\limits_{i=1}^N \\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum\\limits_{i=1}^N \\left(x_i - \\bar{x}\\right)^2} \\\\ &amp;\\hat{a_1} = \\frac{Cov\\left(\\mathbf{X},\\mathbf{Y}\\right)}{Var\\left(\\mathbf{X}\\right)} \\tag{2.16} \\end{align}\\] The fact that the estimate of \\(\\hat{a}_1\\) is the covariance divided by the variance seems like a very interesting result, but why is that the case? If the covariance is the degree to which two variables vary with each other and the variance is the degree to which a single varialbe varies than the ratio of the covariance to the variance is the degree to which two variables vary reduced by the degree to which the input variable varies within itself. If you think back to our discussion of covariance, as the slop increases the covariance decreases, because there is less variation within the input variable, but if we divide by the variance of the input variable this should correct for that diminishing covariance. The following graph calculates the covariance, variance and their ratio for three sets of data points. You can see that as the slop decreases both the covariance and variance increase, but since the covariance is only increasing linearly, while the variance is increasing exponentially, the overall ratio decreases. Putting these two estimates together I can now perform the least squares estimation of simple linear regression. The following plot demonstrates the least squares fit for three sets of points. The code to run this model is discussed below in the section NEED SECTION NAME 2.3 Model Accuracy 2.3.1 Accuracy of the Coefficients While the ability to estimate the coefficients is very powerful, the question is how accurate is the estimation given the data points. You can see from equations (2.15) and (2.16) that the least squares method of linear regression will always give an estimate, but with what confidence can we rely on that estimate? One metric commonly used is the standard error, which is the standard deviation of the sampling distribution. This is not the regular standard deviation which you might be used to. The standard deviation is the degree to which sample vary about the mean and mathematically is the square root of the variance. The standard error is the variation of the sample mean from the population mean, in other words, how close the sample mean is to the true population mean. To get an intuitive understanding of what I mean by variation of the sample mean, I am going to calculate the mean of different size samples drawn from a population of one hundred thousand points. The standard error, in this case specifically the standard error of the mean, is the standard deviation of each set of colored points. Given a population of one hundred thousand data points you can see that the sample size has a large impact on the standard deviation of the sample mean. The greater percentage of points from the population that are used in the sample, the narrower the range of sample means. Just like the standard deviation is the square root of the population variance of the population, so too the standard error is the square root of the sample variance. \\[\\begin{align} SE(\\bar{x}) &amp;= \\sqrt{Var(\\bar{x})} = \\sqrt{s^2} \\\\ &amp;= \\sqrt{\\frac{1}{N-1}\\sum\\limits_{i=1}^N \\left(x_i - \\bar{x}\\right)^2} \\tag{2.17} \\end{align}\\] As matches the graphical intuition, the larger the sample size the smaller the standard error of the sample mean. Given that sampling and sample size affects the mean, we can expect that they should affect our coefficient estimates. Similar to the previous graph, I can estimates the coefficients of multiple sample sets all drawn from a single linear distribution. How can we use this information to estimate the standard error When more input variables are added the simple part of the term is dropped and it is referred to as general regression and takes the form \\[\\begin{equation} Y = a_0 + a_1\\cdot X_1 + a_2\\cdot X_2 + \\ldots a_p\\cdot X_p + \\epsilon = a_0 + \\sum\\limits_{i=1}^p(a_i\\cdot X_i) + \\epsilon \\tag{2.18} \\end{equation}\\] Just as in simple linear regression, the \\(a\\)’s in front of the input variables control the slope of the line, \\(a_0\\) controls the intercept of the line. Taken together the \\(a\\)’s are referred to as the coefficients. \\(\\epsilon\\) is the error as measured by the distance from the points on the right hand side to the true values on the left hand side of the equation. Going back to our original simple linear regression problem involving temperature measurments. Since we know that the relationship between the input and output variables is not strictly goverened by the equation \\(F = \\dfrac{9}{5}\\cdot C + 32\\), we need a method of determining the coefficients which actually govern the relationship between our given input and outputs. 2.4 Variable parameters The values of the input and output parameters in a linear regression model are as follows. - Input Values can be any of the following - numerical values, also referred to as quantitative values - Dummy va You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter ??. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure ??. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table ??. You can write citations, too. For example, we are using the bookdown package [@R-bookdown] in this sample book, which was built on top of R Markdown and knitr [@xie2015]. "],
["statistics.html", "A Statistics A.1 Population Vs. Sample Variances", " A Statistics A.1 Population Vs. Sample Variances The variance given in equation (2.6) is referring to the case where \\(x\\) is a discrete random variable. Discrete in this context means that the variable can take on only a certain number of values as opposed to a continuous random variable, which can take on any value. Almost all data used in modeling, and in fact almost all recorded data is a discrete variable, due to how we record and log data. To understand why this is true, consider recording the amount of rainfall anywhere in the world at any time. In theory this can take on any possible value, and the exact amount of rainfall is a continuous variable, but when we record the rainfall we tend to only record to a certain degree of accuracy, such as millimiters. Even though we might have tens of thousands of different possible values, they are still a finite amount, and thus this variable would be discrete. Getting back to equation (2.6), the general form from which it is derived is as follows: \\[\\begin{equation} Var(\\mathbf{X}) = E\\left[\\left(\\mathbf{X}-\\mu\\right)^2\\right] \\tag{A.1} \\end{equation}\\] What equation (A.1) is saying is that the variance of \\(\\mathbf{X}\\) is equal to the expected value, \\(E\\), of the squared deviation of \\(\\mathbf{X}\\) from its mean, \\(\\mu\\). The expected value in probability is a mathematical short hand for the average value of the variable over many repititions over many calculations of that variable. For example, if you were to keep rolling a perfectly balanced six-sided die the average value over time would get closer and closer to 3.5. If this sounds like the mean, you wouldn’t be wrong, as the mean is just a specialized case of expected value where all probabilites are equal. In a discrete case, where variable \\(\\mathbf{X}\\) can take on any one of a possible \\(n\\) values, then the expected value of $, can be formulated as : \\[\\begin{equation} E\\left[\\mathbf{X}\\right] = x_1p_1 + x_2p_2 + \\ldots + x_np_n = \\sum\\limits_{i=1}^n x_ip_i \\tag{A.2} \\end{equation}\\] Where the expected value is equal to the sum of all possible cases \\(x_1\\) through \\(x_n\\) multiplied by the probability of each case occuring, \\(p_1\\) through \\(p_n\\). When calculating the mean you are just assuming that all probabilites are equal, i.e., \\(p_1 = p_2 = p_n\\). Which reduces equation (A.2) to the form we are more familiar with: \\[\\begin{align} E\\left[\\mathbf{X}\\right] &amp;= \\sum\\limits_{i=1}^n x_ip \\\\ &amp;= \\sum\\limits_{i=1}^n x_i \\frac{1}{n} \\\\ &amp;= \\frac{1}{n} \\sum\\limits_{i=1}^n x_i \\tag{A.3} \\end{align}\\] This same reduction for the mean can be applied to the expected value of the variance as well, assuming that \\(\\mathbf{X}\\) is a discrete variable with each value having equal probability of occuring: \\[\\begin{align} E\\left[\\left(\\mathbf{X}-\\mu\\right)^2\\right] &amp;= \\left(x_1 - \\mu\\right)^2p + \\left(x_2 - \\mu\\right)^2p + \\ldots + \\left(x_n - \\mu\\right)^2p \\\\ &amp;= \\left(x_1 - \\mu\\right)^2\\frac{1}{n} + \\left(x_2 - \\mu\\right)^2\\frac{1}{n} + \\ldots + \\left(x_n - \\mu\\right)^2\\frac{1}{n} \\\\ &amp;= \\sum\\limits_{i=1}^n \\left(x_i - \\mu\\right)^2\\frac{1}{n} \\\\ &amp;= \\frac{1}{n} \\sum\\limits_{i=1}^n \\left(x_i - \\mu\\right)^2 \\tag{A.4} \\end{align}\\] Now that you have an understanding of expected value, I can ask the question, how does the expected value of the variance calculated from the sample mean, termed the sample variance, relate to the variance calculated from the population mean, termed the population variance. \\[\\begin{equation} E\\left[s^2\\right] = E\\left[\\frac{\\sum\\limits_{i=1}^n\\left(x_i - \\bar{x}\\right)^2}{n} \\right] \\Longleftarrow ? \\Longrightarrow \\sigma^2 \\end{equation}\\] Where \\(s^2\\) is the sample variance and \\(\\bar{x}\\) is the notation for the sample mean value of \\(x\\). Before I can answer this questions I need to provide a few more mathematical identities which are necessary for the equations that follow. The first few deal with the properties of expected values and variances, also for the sake of readability I am removing the limits from the \\(\\sum\\limits_{i=1}^n\\) terms. \\[\\begin{equation} \\begin{split} &amp;E\\left[c\\mathbf{X}\\right] = cE\\left[\\mathbf{X}\\right] \\\\ &amp;Var\\left(c\\mathbf{X}\\right) = c^2\\left(\\mathbf{X}\\right) \\\\ \\end{split} \\tag{A.5} \\end{equation}\\] Constants in expected values are just themselves, while for variances they get squared since the variance is a square function. \\[\\begin{equation} \\begin{split} E\\left[\\sum\\mathbf{X}\\right] &amp;= E\\left[x_1 + x_2 + \\ldots + x_n \\right]\\\\ &amp;= E\\left[x_1\\right] + E\\left[x_2\\right] + \\ldots + E\\left[x_n\\right] \\\\ &amp;= \\sum E\\left[\\mathbf{X}\\right]\\\\ Var\\left(\\sum\\mathbf{X}\\right) &amp;= Var\\left(x_1 + x_2 + \\ldots + x_n \\right) \\\\ &amp;= Var\\left(x_1\\right) + Var\\left(x_2\\right) + \\ldots + Var\\left(x_n\\right) \\\\ &amp;= \\sum Var\\left(\\mathbf{X}\\right)\\\\ \\end{split} \\tag{A.6} \\end{equation}\\] For series, expected values and variances operate the same way \\[\\begin{equation} E\\left[E\\left[\\mathbf{X}\\right]\\right] = E\\left[\\mathbf{X}\\right] \\tag{A.7} \\end{equation}\\] The logic behind equation (A.7) is that the expected value of any variable is a constant, and the expected value of a constant is that same constant. So multiple stacked expected values reduce to a single one. I can then use these equations for the population and sample variables. \\[\\begin{equation} \\begin{split} &amp;E\\left[\\mathbf{X}\\right] = E\\left[x_i\\right] = \\mu \\\\ &amp;Var\\left(\\mathbf{X}\\right) = Var\\left(x_i\\right) = \\sigma^2 \\\\ &amp;E\\left[\\sum x_i\\right] = \\sum \\left[x_i\\right] = n\\mu \\\\ &amp;Var\\left(\\sum x_i\\right) = \\sum Var\\left(x_i\\right) = n\\sigma^2 \\\\ &amp;E\\left[\\bar{x}\\right] = E\\left[\\frac{1}{n} \\sum x_i \\right] = \\frac{1}{n} \\sum E\\left[x_i\\right] = \\mu \\\\ &amp;Var\\left(\\bar{x}\\right) = Var\\left(\\frac{1}{n} \\sum x_i \\right) = \\frac{1}{n} \\sum Var\\left(x_i\\right) = \\frac{\\sigma^2}{n} \\\\ \\end{split} \\tag{A.8} \\end{equation}\\] Using the preiovus identities I can now rearrange the population variance in equation (A.1) into a slightly different form. \\[\\begin{align} Var(\\mathbf{X}) &amp;= E\\left[\\left(\\mathbf{X}-\\mu\\right)^2\\right] \\\\ &amp;= E\\left[\\left(\\mathbf{X}-E\\left[\\mathbf{X}\\right]\\right)^2\\right] \\\\ &amp;= E\\left[\\mathbf{X}^2-2\\mathbf{X}E\\left[\\mathbf{X}\\right] + E\\left[\\mathbf{X}\\right]^2\\right] \\\\ &amp;= E\\left[\\mathbf{X}^2\\right] - E\\left[2\\mathbf{X}E\\left[\\mathbf{X}\\right]\\right] + E\\left[E\\left[\\mathbf{X}\\right]^2\\right] \\\\ &amp;= E\\left[\\mathbf{X}^2\\right] - 2E\\left[\\mathbf{X}\\right]^2 + E\\left[\\mathbf{X}\\right]^2 \\\\ &amp;= E\\left[\\mathbf{X}^2\\right] - E\\left[\\mathbf{X}\\right]^2 \\tag{A.9} \\end{align}\\] Now we are finally ready to tackle the question of relating the sample variance to the population variance. Although I am going to cheat a little and move the \\(n\\) from the divisor to the left hand side of the equation. \\[\\begin{align} n E\\left[s^2\\right] &amp;= E\\left[\\sum\\left(x_i - \\bar{x}\\right)^2\\right] \\\\ &amp;= E\\left[\\sum\\left(x_i\\right)^2 - 2 \\bar{x} \\sum \\left(x_i\\right) + \\bar{x}^2\\sum\\left(1\\right)\\right] \\\\ &amp;= E\\left[\\sum\\left(x_i\\right)^2 - 2 n\\bar{x}^2 + n\\bar{x}^2\\right] \\\\ &amp;= E\\left[\\sum\\left(x_i\\right)^2 - n\\bar{x}^2 \\right] \\\\ &amp;= \\sum E\\left[\\left(x_i\\right)^2\\right] - nE\\left[\\bar{x}^2 \\right] \\\\ &amp;= n E\\left[\\left(x_i\\right)^2\\right] - nE\\left[\\bar{x}^2 \\right] \\\\ &amp;= n\\left(Var\\left(x_i\\right) + E\\left[x_i\\right]^2\\right) - n\\left(Var\\left(\\bar{x}\\right) + E\\left[\\bar{x}\\right]^2\\right) \\\\ &amp;= n\\left(\\sigma^2 - \\mu^2\\right) - n\\left(\\frac{1}{n}\\sigma^2 + \\mu^2\\right) \\\\ &amp;= n\\sigma^2 - n\\mu^2 - \\sigma^2 + n\\mu^2 \\\\ &amp;= (n-1)\\sigma^2 \\\\ \\tag{A.10} \\end{align}\\] What this means is that to calculate the population variance using the sample mean you need to divide by \\(n-1\\) instead of just \\(n\\). In fact this is what most programming languages and software do when you calculate the variance of a data set. Using R as an example x &lt;- -3:3 var(x) ## [1] 4.666667 sum((x-mean(x))^2)/(length(x)-1) ## [1] 4.666667 "]
]
