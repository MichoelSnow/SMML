<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistics, Models and Machine Learning</title>
  <meta name="description" content="Statistics, Models and Machine Learning">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Statistics, Models and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistics, Models and Machine Learning" />
  
  
  

<meta name="author" content="Michoel Snow">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  

<link rel="next" href="sim-lin-reg.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#motivation-for-this-book"><i class="fa fa-check"></i><b>1.1</b> Motivation for this book</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i><b>1.2</b> How to read this book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#notation-and-nomeclature-used-in-this-book"><i class="fa fa-check"></i><b>1.3</b> Notation and nomeclature used in this book</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#supervised-vs-unsupervised-models"><i class="fa fa-check"></i><b>1.4</b> Supervised vs Unsupervised Models</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#solving-supervised-models"><i class="fa fa-check"></i><b>1.4.1</b> Solving Supervised Models</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#assessing-model-quality-of-fit"><i class="fa fa-check"></i><b>1.5</b> Assessing Model Quality of Fit</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#mse"><i class="fa fa-check"></i><b>1.5.1</b> MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html"><i class="fa fa-check"></i><b>2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>2.2</b> Estimating the Coefficients</a><ul>
<li class="chapter" data-level="2.2.1" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#least-squares"><i class="fa fa-check"></i><b>2.2.1</b> Least squares</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#accuracy-of-the-coefficients"><i class="fa fa-check"></i><b>2.3.1</b> Accuracy of the Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#variable-parameters"><i class="fa fa-check"></i><b>2.4</b> Variable parameters</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>A</b> Statistics</a><ul>
<li class="chapter" data-level="A.1" data-path="statistics.html"><a href="statistics.html#pop-vs-smp-var"><i class="fa fa-check"></i><b>A.1</b> Population Vs. Sample Variances</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics, Models and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Statistics, Models and Machine Learning</h1>
<h4 class="author"><em>Michoel Snow</em></h4>
</div>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>Should I be using linear regression, logistics regression, SVM, neural networks or one of at least a dozen other statistical/machine learning algorithms when trying to model my data? This is the fundamental question which has driven the authorship of this book. The problem is that the answer to that question for the most part is always the same, it depends. The answer depends not only on the data itself, but also why you are modeling in the first place, i.e., what question are you trying to answer with your model, as well as a whole host of other factors. The purpose of this book is to help you identify the important factors for choosing a model, given those factors which model(s) you should choose and why, and then finally how to implement that model in R or python.</p>
<div id="motivation-for-this-book" class="section level2">
<h2><span class="header-section-number">1.1</span> Motivation for this book</h2>
<p>In undergrad I majored in bioengineering, then for grad school I started doing some basic modeling work in neuroscience. As I was only working with a few models, I really only needed to learn the inner workings of those specific models. However, after grad school when I was interviewing for jobs, they would often follow a similar pattern. Either I would be asked about which modeling approaches I felt comfortable with or about alternative modeling approaches for my research than the ones I used. To say I floundered would be a kind description of my response to these questions. It was obvious that to be competitive I needed to build a real foundation in modeling approaches.</p>
<p>When I started reading books and taking courses, I ran into a different set of issues. The vast majority of these sources were written assuming that the learner was still in midst of schooling and had only recently taken courses like statistics and linear algebra. I had taken those courses over a decade before and my current knowledge of those fields was limited to the techniques that I was using at the time, and even for the the techniques I was still using, I had long forgotten their theoretical underpinnings. Thus this book tries to be complete in the sense that it only assumes the most basic knowledge of statistics and a bit of linear algebra, i.e., vector and matrix multiplication. That being said, I try to approach most concepts from multiple directions so even if vector and matrix operations are not your strong suit you should still come away with at least intuitive understanding of the presented topics.</p>
</div>
<div id="how-to-read-this-book" class="section level2">
<h2><span class="header-section-number">1.2</span> How to read this book</h2>
<p>This book is long on purpose. I strove to make this book a one stop shop for all concepts related to the discussed topics, including multiple derivations and long winded discussions on what some might consider, simple ideas. This is intentional, and as such this book is more intended as a complete reference and I do not expect the average person to read the entire thing straight from start to finish. I have written it so that it can be read from start to finish but I expect most people to jump around to the topics that interest them and skip those that do not. To make this a little easier I have tried to be consistent in how material is presented.</p>
<p>This book is broken up by model, ordered by computational complexity. Every chapter begins with a series of dataset visualizations and associated questions which are best answered through the models described in that chapter. Each chapter is intended to be modular and for the most part, you can read any of the chapters in any order. When multiple models utilize statistics or statistical techniques, the technique will be discussed the first time it is mentioned and all later mentions will refer the reader to where in the book they can find an explanation of that technique.</p>
</div>
<div id="notation-and-nomeclature-used-in-this-book" class="section level2">
<h2><span class="header-section-number">1.3</span> Notation and nomeclature used in this book</h2>
<p>We are all aware of the need for precision in mathematics and computation so as not to create unintentional errors, but I feel the same is true for language, especially in the description of mathematics and computation. For that reason I will strive to be consistent in my use of terminology, use the simplest possible accurate term and use as few terms as possible. For the sake of those who read other books on the subject, whenever a new term is introduced I will try to give all other commonly used names for that term. I will also bold a term whenever I give its definition. Note that when these rules disagree with standard agreed upon nomenclature, I will defer to the standard nomenclature, but will then explicitly define the terms to avoid confusion.</p>
<p>Mathematical modeling lies at the heart of statistical learning, machine learning and any other name dreamt up by a marketing department. So instead of trying to define where one field ends and the other begins, or which technique is machine learning and which is statistics I am simply going to use the term model. In this book any relationship between variables is a <strong>model</strong>. A <strong>variable</strong> is a symbol which contains one or more known or unknown values. When the variable can contain any number of subset variables I will use the bold typeface and capital letters to represent it, such as <span class="math inline">\(\mathbf{X}\)</span> or <span class="math inline">\(\mathbf{Y}\)</span>. Variables which contain no subset variables will be written using the regular typeface and capital letters, such as <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>. Individual values of a variable known as <strong>observations</strong>, also referred to as samples, will be written using a regular typeface and lowercase letters, such as <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>. Indvidual observations for multiple variable will be written using the bold typeface and lowercase letters, such as <span class="math inline">\(\mathbf{x}\)</span> or <span class="math inline">\(\mathbf{y}\)</span>. Numbered subscripts will refer to a specific subset or observation and lettered subscripts will be used for generic subsets or observations. When referring to both individual observations and subset variables, the first subscript is the observation and the second is the variable. The generic total number of observations is <span class="math inline">\(n\)</span> and the generic total number of variables is <span class="math inline">\(p\)</span>.</p>
<p>THis will make more sense when put together in an example. Let us say I am building a model which takes as its first four inputs the GPA, major, minor and high school of every student in Cornell’s graduating class of 1900. <span class="math inline">\(\mathbf{X}\)</span> refers to the set of all input variables. <span class="math inline">\(X_1\)</span> refers to the GPA of all students and <span class="math inline">\(X_2\)</span> refers to their major. <span class="math inline">\(\mathbf{x}_1\)</span> refers to all the inputs for the first student and <span class="math inline">\(\mathbf{x}_i\)</span> refers to the set of inputs for some generic <span class="math inline">\(i\)</span>th student. <span class="math inline">\(x_{3,1}\)</span> refers to the GPA for the third student. You can also think of this as a table where the rows are the observations and the columns are the subset variables. In matrix format this would look like:</p>
<p><span class="math display">\[\mathbf{X} = 
\left(X_1\; X_2\; X_3\; \ldots \; X_p\right)=
\left(\begin{array} {c}
\mathbf{x}_1 \\
\mathbf{x}_2 \\
\mathbf{x}_3 \\
\vdots \\
\mathbf{x}_n
\end{array}\right) = 
\left(\begin{array}
{rrr}
x_{1,1} &amp; x_{1,2} &amp; x_{1,3} &amp; \ldots &amp; x_{1,p}\\
x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; \ldots &amp; x_{2,p}\\
x_{3,1} &amp; x_{3,2} &amp; x_{3,3} &amp; \ldots &amp; x_{3,p}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n,1} &amp; x_{n,2} &amp; x_{n,3} &amp; \ldots &amp; x_{n,p}\\
\end{array}\right)\]</span></p>
<p>Unlike variables which are one or more unknowns, <strong>values</strong> are unknowns which always only take on a single value and will be represented with a lowercase letter, e.g., <span class="math inline">\(a\)</span>. Values which are associated with variables will share their subscripts, e.g., a value, <span class="math inline">\(b\)</span>, associated with variable <span class="math inline">\(X_i\)</span>, will be referenced as <span class="math inline">\(b_i\)</span>. A value associated with a specific variable is referred to as its <strong>coefficient</strong>.</p>
<p>A <strong>function</strong> is a rule for taking an input and returning an output and is represented with a lower case letter followed by a pair of parentheses surrounding the input, e.g., <span class="math inline">\(f(input)\)</span>. Variables which are part of a function’s input are not suprisingly input variables, these are also referred to as indepedent variables, predictors or features. Similarly variables in the output will be termed output variables, but are also called dependent variables or responses. Using the model of a straight line, whose equation is <span class="math display">\[Y = mX + b\]</span> I can rewrite this as a function of the variable <span class="math inline">\(X\)</span> as <span class="math display">\[ f(X) = mX + b\]</span> where <span class="math inline">\(f(X) = Y\)</span>.</p>
<p>When building models often times we are only estimating the value of a coefficient or variable and don’t know its actual value. To distinguish <strong>estimates</strong> of variables and coefficients from their true counterparts, we put tiny hats on them, e.g., <span class="math inline">\(\hat{\mathbf{Y}}\)</span>, <span class="math inline">\(\hat{x}_{i,j}\)</span> or <span class="math inline">\(\hat{a}_0\)</span>.</p>
</div>
<div id="supervised-vs-unsupervised-models" class="section level2">
<h2><span class="header-section-number">1.4</span> Supervised vs Unsupervised Models</h2>
<p>There are many ways to subdivide the various mathematical models, but for the topics covered within this book the dichotomy is made between supervised and unsupervised models. In Supervised models the data usually consists of observations where for every observation there is a paired input and output. In supervised modeling you are often given a training dataset which consists of entries for both the inputs and the outputs, as well as a test dataset, which only contains inputs from which you must predict outputs. The goal in supervised modeling is to create a function which accurately transforms the inputs into the outputs. In general the output only consists of a single variable but the input can consist of one or more variables. The standard nomenclature is to use one symbol for the output and one symbol for the input.</p>
<p>In unsupervised learning the data is unlabeled, i.e., it is not split into inputs and outputs and the goal is to learn some desired feature about the data. A classic problem in unsupervised learning, is clustering, wherein the goal is to split the data into groups such that data within each group is more similar to each other than to data in other groups.</p>
<div id="solving-supervised-models" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Solving Supervised Models</h3>
<p>In general, the goal of supervised models is to design a function which can estimate a desired output variable <span class="math inline">\(\mathbf{Y}\)</span>, given a set of input variables <span class="math inline">\(\mathbf{X}\)</span>, either for the puproses of predicting future outputs or for understanding the relationship between the inputs and the outputs. All supervised models assume that there is some relationship between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> such that the output variable <span class="math inline">\(\mathbf{Y}\)</span> is a function of some fixed, but unknown function of <span class="math inline">\(\mathbf{X}\)</span>, written as <span class="math inline">\(f(\mathbf{X})\)</span>:</p>
<span class="math display" id="eq:SupModApprox">\[\begin{equation}
  \mathbf{Y} \approx f(\mathbf{X}) 
  \tag{1.1}
\end{equation}\]</span>
<p>In equation <a href="index.html#eq:SupModApprox">(1.1)</a>, <span class="math inline">\(\mathbf{Y}\)</span> is only approximately equal to <span class="math inline">\(f(\mathbf{X})\)</span> because there almost always exists some random error in data. This is true whether we are talking about neural activity, weather patterns or economics. Nothing behaves exaclty as expected, instead there is always some random error. I can formalize this error by adding an error term, <span class="math inline">\(\epsilon\)</span>, to equation <a href="index.html#eq:SupModApprox">(1.1)</a>.</p>
<span class="math display" id="eq:SupMod">\[\begin{equation}
  \mathbf{Y} = f(\mathbf{X}) + \epsilon
  \tag{1.2}
\end{equation}\]</span>
<p>Where <span class="math inline">\(\epsilon\)</span> represents the inherent random error, is assumed to be indepedent and has an assumed mean of zero. In this case independence means that the error is unaffected by the input variable (even though this is not always true). Assuming a zero mean error implies that there is no inherent bias in the error, e.g., the error will not shift the data in one direction more than any others. The implication of <span class="math inline">\(\epsilon\)</span> is that even if we are able to perfectly estimate <span class="math inline">\(f(\mathbf{X})\)</span>, we would be unable to predict <span class="math inline">\(\mathbf{Y}\)</span> with perfect accuracy due to the inherent error which is indepedent of, and so cannot be predicted by, <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>However, the problem is that while we assume this relationship exists, we do not know what the function <span class="math inline">\(f(\mathbf{X})\)</span> is. To that end, supervised models seek to estimate this function using data consisting of inputs values paied with corresponding output values. When used to estimated the parameters of a model, this dataset if often referred to as <strong>training data</strong>. This is in contrast to <strong>test data</strong>, which as the name states, is used to test the accuracy of the model on data different than what it was trained on. Using the estimate of the function learned from the training data we are able to make predictions of the output values given the input values in the test data.</p>
<span class="math display" id="eq:SupModEst">\[\begin{equation}
  \hat{\mathbf{Y}} = \hat{f}(\mathbf{X})
  \tag{1.3}
\end{equation}\]</span>
<p>Note that both the function and the output variable are indicated as estimates (through their tiny hats), but the input variable is not indicated as an estimate. This is because we know what the true inputs are we just don’t know what the function that relates the inputs to the outputs truly is and following from that we do not know what the output truly is for any given input, hence the tiny hats.</p>
<p>The most common features which distinguish the different supervised models are as follows</p>
<ul>
<li>The mathematical structure of <span class="math inline">\(f(\mathbf{X})\)</span><br />
</li>
<li>The type of data that make up the output variable</li>
<li>Estimation method of <span class="math inline">\(f(\mathbf{X})\)</span></li>
</ul>
</div>
</div>
<div id="assessing-model-quality-of-fit" class="section level2">
<h2><span class="header-section-number">1.5</span> Assessing Model Quality of Fit</h2>
<div id="mse" class="section level3">
<h3><span class="header-section-number">1.5.1</span> MSE</h3>
<p>** START FROM HERE** However, note that unlike equation <a href="index.html#eq:SupMod">(1.2)</a> When estimating the function <span class="math inline">\(f()\)</span>, there are two types of errors to consider</p>
<!-- ```{r, echo=FALSE, warning=FALSE} -->
<!-- library(NHANES) -->
<!-- NH_smp <- NHANES[sample(nrow(NHANES),500),] -->
<!-- Age <- NH_smp$Age -->
<!-- Height <- NH_smp$Height -->
<!-- fit <- lm((Height)~ Age) -->
<!-- fit.pred <- (predict(fit,data.frame(Age=min(Age):max(Age)))) -->
<!-- NH_2 <- tibble(Age=min(Age):max(Age),Height=fit.pred ) -->
<!-- NH_smp %>% ggplot(aes(Age,Height)) + geom_jitter() +geom_line(data=NH_2,aes(Age,Height)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- library(ElemStatLearn) -->
<!-- library(tidyverse) -->
<!-- mrk <- as_tibble(marketing) -->
<!-- mrk <- mrk %>% mutate(Inc = 10000 + (Income-1)*5000) -->
<!-- mrk$Inc[mrk$Income==6] <- 40000 -->
<!-- mrk$Inc[mrk$Income==7] <- 50000 -->
<!-- mrk$Inc[mrk$Income==8] <- 75000 -->
<!-- mrk$Inc[mrk$Income==9] <- 90000 -->
<!-- mrk %>% ggplot(aes(Edu,Income)) + geom_point() -->
<!-- ``` -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="sim-lin-reg.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
