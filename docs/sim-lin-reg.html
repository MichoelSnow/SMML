<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistics, Models and Machine Learning</title>
  <meta name="description" content="Statistics, Models and Machine Learning">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Statistics, Models and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistics, Models and Machine Learning" />
  
  
  

<meta name="author" content="Michoel Snow">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="mult-lin-reg.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#motivation-for-this-book"><i class="fa fa-check"></i><b>1.1</b> Motivation for this book</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#how-to-read-this-book"><i class="fa fa-check"></i><b>1.2</b> How to read this book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#notation-and-nomeclature-used-in-this-book"><i class="fa fa-check"></i><b>1.3</b> Notation and nomeclature used in this book</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#supervised-vs-unsupervised-models"><i class="fa fa-check"></i><b>1.4</b> Supervised vs Unsupervised Models</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#solving-supervised-models"><i class="fa fa-check"></i><b>1.4.1</b> Solving Supervised Models</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#assessing-model-quality-of-fit"><i class="fa fa-check"></i><b>1.5</b> Assessing Model Quality of Fit</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#mse"><i class="fa fa-check"></i><b>1.5.1</b> MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html"><i class="fa fa-check"></i><b>2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>2.2</b> Estimating the Coefficients</a><ul>
<li class="chapter" data-level="2.2.1" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#least-squares"><i class="fa fa-check"></i><b>2.2.1</b> Least squares</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#model-accuracy"><i class="fa fa-check"></i><b>2.3</b> Model Accuracy</a><ul>
<li class="chapter" data-level="2.3.1" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#accuracy-of-the-coefficients"><i class="fa fa-check"></i><b>2.3.1</b> Accuracy of the Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="sim-lin-reg.html"><a href="sim-lin-reg.html#variable-parameters"><i class="fa fa-check"></i><b>2.4</b> Variable parameters</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mult-lin-reg.html"><a href="mult-lin-reg.html"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="mult-lin-reg.html"><a href="mult-lin-reg.html#overview-1"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>A</b> Statistics</a><ul>
<li class="chapter" data-level="A.1" data-path="statistics.html"><a href="statistics.html#pop-vs-smp-var"><i class="fa fa-check"></i><b>A.1</b> Population Vs. Sample Variances</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics, Models and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sim-lin-reg" class="section level1">
<h1><span class="header-section-number">2</span> Simple Linear Regression</h1>
<div id="overview" class="section level2">
<h2><span class="header-section-number">2.1</span> Overview</h2>
<p>Almst all supervised models relate one or more input variables, <span class="math inline">\(X_1,X_2, \ldots , X_p\)</span>, to an output variable, <span class="math inline">\(Y\)</span>, which can be generalized as <span class="math display">\[Y = f(X)\]</span> The goal of these models is to estimate the function <span class="math inline">\(f(X)\)</span> with as much accuracy as possible. In a <strong>linear regression</strong> model, the assumption is that the input variables are related to the output variables through a linear combination, i.e., addition. The term regression refers to the fact that the output variable can take on any value. This is in contrast to classification models in which the output variable can take on one of a limited set of values. Classification models will be discussed in chapter <strong>NEED CHAPTER NAME</strong>. When the model is estimating the function for only a single input variable, it is termed simple linear regression and takes the form</p>
<span class="math display" id="eq:SimpLinReg">\[\begin{equation} 
  Y = a_0 + a_1X
  \tag{2.1}
\end{equation}\]</span>
<p>This equation might seem somewhat familiar to you, and by just replacing the letters used for the terms and switching around the order I end up with an equation which we all have encountered in grade school as the equation for a line</p>
<span class="math display">\[\begin{equation} 
  y = mx + b
\end{equation}\]</span>
<p>Who knew we were all doing a form of linear regression so many years ago. Even though the letters used for simple linear regression are different they have the same meaning. <span class="math inline">\(a_0\)</span> is the intercept and <span class="math inline">\(a_1\)</span> is the slope of the line. However, unlike in math class where the line perfectly fit the points we were given, when dealing with data points in the real world this is almost never true. This is the reason that I keep using the word estimate. The model might be close but there should always be the expectation that it will not prefectly fit the data. Additionally, if you try to perfectly fit the data you run the risk of overfitting your model, a topic which will be discussed in chapter <strong>NEED CHAPTER NAME</strong>. So when discussing models applied to real world data the equation for simple linear regression is often written as</p>
<span class="math display" id="eq:SimpLinRegapprox">\[\begin{equation} 
  Y \approx a_0 + a_1 X
  \tag{2.2}
\end{equation}\]</span>
<p>To understand why equation <a href="sim-lin-reg.html#eq:SimpLinRegapprox">(2.2)</a> is more appropriate than equation <a href="sim-lin-reg.html#eq:SimpLinReg">(2.1)</a>, let’s look at some data. Plotted in the figure below are average temperature readings at JFK airport from January through July of 2017.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Using the least squares method described below I can estimate the linear regression coefficients and overlay the best fit line.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>You can see that there is no straight line which would hit all the points, so going back to my original assertion, even the most accurate model could only approximate the output variable given the input variables.</p>
<p>Taking another step back, we have to remember that linear regression is a method of estimating the efficients of equation <a href="sim-lin-reg.html#eq:SimpLinRegapprox">(2.2)</a>, which I can represent by giving the estimated coefficients and estimate output variables tiny hats.</p>
<span class="math display" id="eq:SimpLinRegHat">\[\begin{equation} 
  \hat{Y} = \hat{a}_0 + \hat{a}_1 X
  \tag{2.3}
\end{equation}\]</span>
</div>
<div id="estimating-the-coefficients" class="section level2">
<h2><span class="header-section-number">2.2</span> Estimating the Coefficients</h2>
<div id="least-squares" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Least squares</h3>
<p>The general idea of the least squares method is that you want to pick coefficents which minimize the differences between the given output and the output calculated from the right hand side of equation <a href="sim-lin-reg.html#eq:SimpLinRegHat">(2.3)</a>. This difference between the calculated output variable and the actual output variable is called the <strong>residual</strong> which is represented by the symbol <span class="math inline">\(e\)</span>.</p>
<span class="math display" id="eq:resid">\[\begin{equation}
  e = Y-\hat{Y} = Y - \left(\hat{a}_0  + \hat{a}_1 X\right)
  \tag{2.4}
\end{equation}\]</span>
<p>The residuals in our previous plot are the distances from the points to the fitted line.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If I then take the square of the residual for each observation and sum them all together I get the <strong>residual sum of squares (RSS)</strong>. Minimizing the RSS is what makes this process “least squares”.</p>
<span class="math display" id="eq:RSS">\[\begin{equation}
  RSS = e_1^2 + e_2^2 + \ldots + e_n^2 = \sum\limits_{i=1}^N e_i^2   
  \tag{2.5}
\end{equation}\]</span>
<p><span class="math inline">\(e_i\)</span> represents the <span class="math inline">\(i\)</span>th residual and refers to the difference between the <span class="math inline">\(i\)</span>th actual output, <span class="math inline">\(Y_i\)</span>, and the <span class="math inline">\(i\)</span>th predicted output, <span class="math inline">\(\hat{Y}_i\)</span>. In the following section I am going to go, step by step, through two different methods of deriving the least squares coefficients. I find derivations useful for two reaons. The first is that they let you know where the equations come from. If you are reading this book then you are looking to understand modeling on a level deeper than simply, “how do I apply a certain model to my data”. The second reason that I like derivations is that you see the assumptions made in the derivations. This, in my opinion, is essential to being able to smartly use the different models, as you understand when the assumptions are valid and when they are not.</p>
<p>There are multiple methods to solve for the minimum least squares coefficients, most dealing with variations on ordinary least squares using matrix algebra. I will be using matrix algebra to solve for the coefficients when there are multiple inputs but for simple linear regression the following derivation only requires knowledge of partial derivatives.</p>
<div id="derivation-1" class="section level4 unnumbered">
<h4>Derivation 1</h4>
<p>Before I start working through the derivation, there are two concepts I want to go through variance and covariance. The reason being that at the end of the derivation the estimate of slope, otherwise known as <span class="math inline">\(a_1\)</span> can be calculated by dividing the covariance by the variance. When I get to that point I will discuss why this is the case, but I first wanted to make sure that you had a firm grasp on these concepts by themselves. Also, the in depth examination of variance will inform later parts of the derivation.</p>
<p>The <strong>variance</strong> is a measure of spread for a variable, i.e., the distance of each value from the sample mean value. An intuitive example is shown below where the variance of the points on the targets increases from left to right.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Mathematically the variance, in one dimension, is calculated as follows:</p>
<span class="math display" id="eq:PopVarDisc">\[\begin{equation}
  Var(x) = \sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N \left(x_i - \mu\right)\left(x_i - \mu\right)  = \frac{1}{N}\sum\limits_{i=1}^N \left(x_i - \mu\right)^2 
  \tag{2.6}
\end{equation}\]</span>
<p>Where <span class="math inline">\(\mu_x\)</span> is the notation for the population mean value of x, which is discussed in the next paragraph. Squaring the difference serves two functions, the first is to equally weight positive and negative contributions to the variance. Without the square if a variable was evenly distributed around zero, regardless of its spread it would have a variance of zero. The second consequence is that as the distances increase the variance increases exponentially, as seen in the following graph:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>However, there is a problem with using equation <a href="sim-lin-reg.html#eq:PopVarDisc">(2.6)</a>, it requires that I already know the population mean. The <strong>population mean</strong> is the mean value given a set of numbers or values. The issue is that most of the time with data we are dealing with a small subsample of data and not the full set of data. For example, if I wanted to calculate the average rainfall in New York City yesterday, I would need to measure the rainfall in every location throughout the city. This is obviously impossible, which is why some people refer to the population mean as the omniscient mean. Instead, what I can do is measure the rainfall at certain points throughout the city and estimate the population mean using the <strong>sample mean</strong>, which is simply the mean calculated from the population sample collected. The difference between population mean and sample mean, might seem like simply a matter of semantics, but it has important ramifications and necessitates corrections. The variance using the sample mean, termed the sample variance, very similar to</p>
<span class="math display" id="eq:SmpVarDisc">\[\begin{equation}
  Var(\bar{x}) = s^2 = \frac{1}{N-1}\sum\limits_{i=1}^N \left(x_i - \bar{x}\right)\left(x_i - \bar{x}\right)  = \frac{1}{N-1}\sum\limits_{i=1}^N \left(x_i - \bar{x}\right)^2 
  \tag{2.7}
\end{equation}\]</span>
<p>where <span class="math inline">\(s^2\)</span> is the sample variance and <span class="math inline">\(\bar{x}\)</span> is the sample mean. For a full derivation of equation <a href="sim-lin-reg.html#eq:SmpVarDisc">(2.7)</a> see <a href="statistics.html#pop-vs-smp-var">Population Vs. Sample Variances</a>.</p>
<p>Unlike the variance which is a measure of how a single variable varies with regards to itself, the <strong>covariance</strong> is a measure of how two variables, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span>, vary with respect to each other. If the variables increase in value together and decrease together, i.e., when one variable increases the other variable increases as well, the covariance will be larger. If instead one variable increases while the other decreases then the covariance will be smaller. To actually calculate the covariance you can use the following formula:</p>
<span class="math display" id="eq:PopCovDisc">\[\begin{equation}
  Cov(x) = \frac{1}{N}\sum\limits_{i=1}^N \left(x_i - \mu_x\right)\left(y_i - \mu_y\right)
  \tag{2.8}
\end{equation}\]</span>
<p>If equation <a href="sim-lin-reg.html#eq:PopCovDisc">(2.8)</a> looks suspiciously like equation <a href="sim-lin-reg.html#eq:PopVarDisc">(2.6)</a>, that’s because the formula for the variance is just a special case of the formula for the covariance when dealing with a single variable. To get an intuitive sense of what the covariance represents here are the covariances for a few different series of points.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As you can see, as the slope of the line increases from the red to the green to the blue points, the covariance decreases. This is because for the red points, a change in <span class="math inline">\(y\)</span> value of 5 roughly corresponds to a change in <span class="math inline">\(x\)</span> value of 7 or 8, but for the green points, the same change in <span class="math inline">\(y\)</span> of 5 corresponds to a change in <span class="math inline">\(x\)</span> of only about 3 or 4. What this means is that for an increased slope the <span class="math inline">\(x\)</span> values are changing at a slower rate and so this decreases their term in the covariance formulation. The purple points on the right show what happens to the covariance when the tyhe two variables move in opposite direction. The magnitutde is the same for the purple and red points, but while the red covariance is positive the purple is negative. This is because, with respect to each variable’s mean value, while <span class="math inline">\(x\)</span> is positive, <span class="math inline">\(y\)</span> is negative and vice versa.</p>
<p>Just like with the variance, I have to distinguish between the population covariance, equation <a href="sim-lin-reg.html#eq:PopCovDisc">(2.8)</a>, and the sample covariance:</p>
<span class="math display" id="eq:SmpCovDisc">\[\begin{equation}
  Cov(x) = \frac{1}{N-1}\sum\limits_{i=1}^N \left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)
  \tag{2.9}
\end{equation}\]</span>
<p>The main difference once again is the replacement of <span class="math inline">\(n-1\)</span> in the denominator in lieu of <span class="math inline">\(n\)</span>, for the same reason as the variance.</p>
<p>Now that I have gone over the concepts of variance and covariance, I just need to go over some quick identities before I derive the least squares coefficients.</p>
<span class="math display" id="eq:SumMn">\[\begin{equation}
\begin{split}
  \frac{1}{N}\sum\limits_{i=1}^N y_i &amp; = \bar{y} \Longleftrightarrow \sum\limits_{i=1}^N y_i &amp; = N\bar{y} \\
  \frac{1}{N}\sum\limits_{i=1}^N x_i &amp; = \bar{x} \Longleftrightarrow \sum\limits_{i=1}^N x_i &amp; = N\bar{x} \\
\end{split}
\tag{2.10}
\end{equation}\]</span>
<p>Equation <a href="sim-lin-reg.html#eq:SumMn">(2.10)</a> is just a simple variation on the definition of <span class="math inline">\(\bar{X}\)</span>. The next two sets of identities are expansions of the variance and covariance equations and the equivalent ways they appear. I am going to be removing the denominator for the sake of simplicity, also because as you will soon see it will be divided out in the final derivation.</p>
<span class="math display" id="eq:VarEq">\[\begin{align}
  \sum\limits_{i=1}^N (x_i-\bar{x})(x_i - \bar{x}) &amp;= \sum\limits_{i=1}^N x_i^2 - 2\sum\limits_{i=1}^N \bar{x}x_i + \sum\limits_{i=1}^N \bar{x}^2 \\
  &amp;= \sum\limits_{i=1}^N x_i^2 - 2N\bar{x}^2 + N\bar{x}^2 \\
  &amp;= \sum\limits_{i=1}^N x_i^2 - \sum\limits_{i=1}^N  \bar{x}x_i = \sum\limits_{i=1}^N x_i\left(x_i-\bar{x}\right) \tag{2.11}
\end{align}\]</span>
<p>Now repeating the process for the covariance:</p>
<span class="math display" id="eq:CovEq2" id="eq:CovEq1">\[\begin{align}
  \sum\limits_{i=1}^N (x_i-\bar{x})(y_i - \bar{y}) &amp;= \sum\limits_{i=1}^N x_iy_i - \sum\limits_{i=1}^N \bar{x}y_i - \sum\limits_{i=1}^N \bar{y}x_i + \sum\limits_{i=1}^N\bar{x}\bar{y}\\
  &amp;=\sum\limits_{i=1}^N x_iy_i - \bar{x}\sum\limits_{i=1}^N y_i - \bar{y} \sum\limits_{i=1}^N x_i + \bar{x}\bar{y}\sum\limits_{i=1}^N 1 \\
  &amp;=\sum\limits_{i=1}^N x_iy_i -N\bar{x}\bar{y} - N\bar{x}\bar{y} + N\bar{x}\bar{y} \\
  &amp;=\sum\limits_{i=1}^N x_iy_i - N\bar{x}\bar{y} \\
  &amp;=\sum\limits_{i=1}^N x_iy_i - \sum\limits_{i=1}^N \bar{y}x_i = \sum\limits_{i=1}^N x_i\left(y_i - \bar{y}\right) \tag{2.12}\\
  &amp;= \sum\limits_{i=1}^N x_iy_i - \sum\limits_{i=1}^N \bar{x}y_i = \sum\limits_{i=1}^N y_i\left(x_i - \bar{x}\right) \tag{2.13}\\
\end{align}\]</span>
<p>This series of equations is simply going back and forth between equivalent values using the relationships established in equation <a href="sim-lin-reg.html#eq:SumMn">(2.10)</a>. I can split and rejoin summations because the operations are all linear. At the end of the above series I end up with two relationships which will be needed later.</p>
<p>Now with all these tools, let’s attack this derivation. If you remember back from your high school and college math classes, when we are trying to find the minimum value of a function we take the derivative, set it equal to zero and solve for the parameter(s) of interest. Thankfully, the process hasn’t changed since then. Also, since there are multiple parameters of interest, i.e., <span class="math inline">\(\hat{a_0}\)</span> and <span class="math inline">\(\hat{a_1}\)</span>, I will need to use partial derivatives. First I want to rewrite equation <a href="sim-lin-reg.html#eq:RSS">(2.5)</a> in a slightly more derivative friendly way.</p>
<span class="math display" id="eq:RSSDer">\[\begin{equation}
  S = \sum\limits_{i=1}^N \left(y_i - \hat{a}_0  + \hat{a}_1 x_i \right)^2   
  \tag{2.14}
\end{equation}\]</span>
<p>Starting with <span class="math inline">\(\hat{a_0}\)</span></p>
<span class="math display" id="eq:a0Deriv">\[\begin{align}
  \frac{\partial S}{\partial \hat{a}_0} = -2 &amp;\sum\limits_{i=1}^N \left(y_i - \hat{a}_0 - \hat{a_1}x_i\right)\\
  &amp;\sum\limits_{i=1}^N \left(y_i - \hat{a}_0 - \hat{a_1}x_i\right) = 0 \\
  &amp;\sum\limits_{i=1}^N y_i - \hat{a}_0 \sum\limits_{i=1}^N 1 - \hat{a_1} \sum\limits_{i=1}^N x_i = 0 \\
  &amp; N\bar{y} - N\hat{a}_0  - N\hat{a_1}\bar{x}  = 0 \\
  &amp; \bar{y} - \hat{a}_0  - \hat{a_1}\bar{x}  = 0 \\
  &amp; \hat{a}_0 = \bar{y} - \hat{a_1}\bar{x}  \tag{2.15}
\end{align}\]</span>
<p>The outcome all these steps is something that seems relatively simple, that the estimate of the intercept is just based on the estimated slope and the average input and output values. That being said it does make sense, as the intercept serves to horizontally shift the line. Once we have estimated the slope we want an intercept which positions the resulting line to go through the center of data as calculated by the average position. Now that I have estimated <span class="math inline">\(\hat{a}_0\)</span>, I can estimate <span class="math inline">\(\hat{a}_1\)</span></p>
<span class="math display" id="eq:a1Deriv">\[\begin{align}
  \frac{\partial S}{\partial \hat{a}_1} = -2 &amp;\sum\limits_{i=1}^N x_i \left(y_i - \hat{a}_0 - \hat{a_1}x_i\right)\\
  &amp;\sum\limits_{i=1}^N x_i \left(y_i - \hat{a}_0 - \hat{a_1}x_i\right) = 0 \\
  &amp;\sum\limits_{i=1}^N x_iy_i - \hat{a}_0 \sum\limits_{i=1}^N x_i - \hat{a_1} \sum\limits_{i=1}^N x_i^2 = 0 \\
  &amp;\sum\limits_{i=1}^N x_iy_i = \hat{a}_0 \sum\limits_{i=1}^N x_i + \hat{a_1} \sum\limits_{i=1}^N x_i^2 \\
  &amp;\sum\limits_{i=1}^N x_iy_i = N\hat{a}_0 \bar{x} + \hat{a_1} \sum\limits_{i=1}^N x_i^2 \\
  &amp;\sum\limits_{i=1}^N x_iy_i = N\left(\bar{y} - \hat{a_1}\bar{x}\right)\bar{x} + \hat{a_1} \sum\limits_{i=1}^N x_i^2 \\
  &amp;\sum\limits_{i=1}^N x_iy_i = N\bar{y}\bar{x} - N\hat{a_1}\bar{x}^2 + \hat{a_1} \sum\limits_{i=1}^N x_i^2 \\
  &amp;\sum\limits_{i=1}^N x_iy_i = \sum\limits_{i=1}^N y_i \bar{x} - \hat{a_1}\sum\limits_{i=1}^N x_i \bar{x} + \hat{a_1} \sum\limits_{i=1}^N x_i^2 \\
  &amp;\sum\limits_{i=1}^N x_iy_i - \sum\limits_{i=1}^N y_i \bar{x} =  \hat{a_1} \sum\limits_{i=1}^N x_i^2 - \hat{a_1}\sum\limits_{i=1}^N x_i \bar{x} \\
  &amp;\sum\limits_{i=1}^N y_i\left(x_i - \bar{x}\right) = \hat{a_1}\sum\limits_{i=1}^N x_i \left(x_i - \bar{x}\right) \\
  &amp;\sum\limits_{i=1}^N \left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right) = \hat{a_1}\sum\limits_{i=1}^N \left(x_i - \bar{x}\right)^2 \\
  &amp;\hat{a_1} = \frac{\sum\limits_{i=1}^N \left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum\limits_{i=1}^N \left(x_i - \bar{x}\right)^2} \\
  &amp;\hat{a_1} = \frac{Cov\left(\mathbf{X},\mathbf{Y}\right)}{Var\left(\mathbf{X}\right)} \tag{2.16}
\end{align}\]</span>
<p>The fact that the estimate of <span class="math inline">\(\hat{a}_1\)</span> is the covariance divided by the variance seems like a very interesting result, but why is that the case? If the covariance is the degree to which two variables vary with each other and the variance is the degree to which a single varialbe varies than the ratio of the covariance to the variance is the degree to which two variables vary reduced by the degree to which the input variable varies within itself. If you think back to our discussion of covariance, as the slop increases the covariance decreases, because there is less variation within the input variable, but if we divide by the variance of the input variable this should correct for that diminishing covariance. The following graph calculates the covariance, variance and their ratio for three sets of data points.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>You can see that as the slop decreases both the covariance and variance increase, but since the covariance is only increasing linearly, while the variance is increasing exponentially, the overall ratio decreases.</p>
<p>Putting these two estimates together I can now perform the least squares estimation of simple linear regression. The following plot demonstrates the least squares fit for three sets of points. The code to run this model is discussed below in the section <strong>NEED SECTION NAME</strong></p>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="model-accuracy" class="section level2">
<h2><span class="header-section-number">2.3</span> Model Accuracy</h2>
<div id="accuracy-of-the-coefficients" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Accuracy of the Coefficients</h3>
<p>While the ability to estimate the coefficients is very powerful, the question is how accurate is the estimation given the data points. You can see from equations <a href="sim-lin-reg.html#eq:a0Deriv">(2.15)</a> and <a href="sim-lin-reg.html#eq:a1Deriv">(2.16)</a> that the least squares method of linear regression will always give an estimate, but with what confidence can we rely on that estimate? One metric commonly used is the <strong>standard error</strong>, which is the standard deviation of the sampling distribution. This is not the regular standard deviation which you might be used to. The <strong>standard deviation</strong> is the degree to which sample vary about the mean and mathematically is the square root of the variance. The standard error is the variation of the sample mean from the population mean, in other words, how close the sample mean is to the true population mean. To get an intuitive understanding of what I mean by variation of the sample mean, I am going to calculate the mean of different size samples drawn from a population of one hundred thousand points.</p>
<p>The standard error, in this case specifically the standard error of the mean, is the standard deviation of each set of colored points. Given a population of one hundred thousand data points you can see that the sample size has a large impact on the standard deviation of the sample mean. The greater percentage of points from the population that are used in the sample, the narrower the range of sample means. Just like the standard deviation is the square root of the population variance of the population, so too the standard error is the square root of the sample variance.</p>
<span class="math display" id="eq:StdErrMn">\[\begin{align}
  SE(\bar{x})  &amp;= \sqrt{Var(\bar{x})} = \sqrt{s^2} \\
  &amp;= \sqrt{\frac{1}{N-1}\sum\limits_{i=1}^N \left(x_i - \bar{x}\right)^2} 
  \tag{2.17}
\end{align}\]</span>
<!-- \begin{align} -->
<!--   SE(\bar{x}) &= \sqrt{Var(\bar{x})} \\ -->
<!--   &= \sqrt{Var\left(\frac{x_1 + x_2 + \ldots + x_n}{n}\right)} \\ -->
<!--   &= \sqrt{\frac{1}{n^2}\left(Var(x_1) + Var(x_2) + \ldots + Var(x_n)\right)} \\ -->
<!--   &= \sqrt{\frac{1}{n^2}\left(\sigma^2 + \sigma^2 + \ldots + \sigma^2\right)} \\ -->
<!--   &= \sqrt{\frac{1}{n^2}n\sigma^2} \\ -->
<!--   &= \sqrt{\frac{\sigma^2}{n}} \\ -->
<!--   &= \frac{\sigma}{\sqrt{n}} (\#eq:StdErrMn) -->
<!-- \end{align} -->
<p>As matches the graphical intuition, the larger the sample size the smaller the standard error of the sample mean. Given that sampling and sample size affects the mean, we can expect that they should affect our coefficient estimates. Similar to the previous graph, I can estimates the coefficients of multiple sample sets all drawn from a single linear distribution.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>How can we use this information to estimate the standard error</p>
<!-- The red bars from the points to the line represent the distance from the real values to the measured values, also know as the error.  So I can rewrite Equation \@ref(eq:SimpLinReg) in one of two ways to be accurate.  The first is to include an error term, $\epsilon$ by standard notation, whose values are the lengths of the individual red lines.   -->
<!-- \begin{equation}  -->
<!--   Y = a_0 + a_1 \cdot X_1 + \epsilon -->
<!--   (\#eq:SimpLinRegErr) -->
<!-- \end{equation}  -->
<!-- $\epsilon$ is a variable with the same number of values as $X$ and $Y$, as seen in the table below -->
<p>When more input variables are added the simple part of the term is dropped and it is referred to as general regression and takes the form</p>
<span class="math display" id="eq:LinReg">\[\begin{equation} 
  Y = a_0 + a_1\cdot X_1 + a_2\cdot X_2 + \ldots a_p\cdot X_p + \epsilon = a_0 + \sum\limits_{i=1}^p(a_i\cdot X_i) + \epsilon
  \tag{2.18}
\end{equation}\]</span>
<p>Just as in simple linear regression, the <span class="math inline">\(a\)</span>’s in front of the input variables control the slope of the line, <span class="math inline">\(a_0\)</span> controls the intercept of the line. Taken together the <span class="math inline">\(a\)</span>’s are referred to as the coefficients. <span class="math inline">\(\epsilon\)</span> is the error as measured by the distance from the points on the right hand side to the true values on the left hand side of the equation.</p>
<p>Going back to our original simple linear regression problem involving temperature measurments. Since we know that the relationship between the input and output variables is not strictly goverened by the equation <span class="math inline">\(F = \dfrac{9}{5}\cdot C + 32\)</span>, we need a method of determining the coefficients which actually govern the relationship between our given input and outputs.</p>
</div>
</div>
<div id="variable-parameters" class="section level2">
<h2><span class="header-section-number">2.4</span> Variable parameters</h2>
<p>The values of the input and output parameters in a linear regression model are as follows. - Input Values can be any of the following - numerical values, also referred to as quantitative values - Dummy va</p>
<p>You can label chapter and section titles using <code>{#label}</code> after them, e.g., we can reference Chapter <a href="#intro"><strong>??</strong></a>. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter <a href="#methods"><strong>??</strong></a>.</p>
<p>Figures and tables with captions will be placed in <code>figure</code> and <code>table</code> environments, respectively.</p>
<p>Reference a figure by its code chunk label with the <code>fig:</code> prefix, e.g., see Figure <a href="#fig:nice-fig"><strong>??</strong></a>. Similarly, you can reference tables generated from <code>knitr::kable()</code>, e.g., see Table <a href="#tab:nice-tab"><strong>??</strong></a>.</p>
<p>You can write citations, too. For example, we are using the <strong>bookdown</strong> package <span class="citation">[@R-bookdown]</span> in this sample book, which was built on top of R Markdown and <strong>knitr</strong> <span class="citation">[@xie2015]</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mult-lin-reg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
